# Full Comprehensive Sweep Configuration
# Complete evaluation across all RoPE methods, context lengths, and benchmarks

# Model Configuration
model:
  type: "hf_hub"
  name: "llama-3.2-1b"
  path: "unsloth/Llama-3.2-1B"
  tokenizer_path: "unsloth/Llama-3.2-1B"
  device_map: "auto"
  torch_dtype: "bfloat16"
  attn_implementation: "eager"
  max_memory_gb: 24
  max_length: 32768
  trust_remote_code: false

# RoPE Scaling Methods to Evaluate (all methods)
rope_methods:
  - name: "none"
    config: {}
  - name: "linear"
    config:
      scaling_factor: 2.0
  - name: "linear"
    config:
      scaling_factor: 4.0
  - name: "linear"
    config:
      scaling_factor: 8.0
  - name: "ntk_aware"
    config:
      scaling_factor: 2.0
      alpha: 4.0
  - name: "ntk_aware"
    config:
      scaling_factor: 2.0
      alpha: 8.0
  - name: "ntk_aware"
    config:
      scaling_factor: 4.0
      alpha: 8.0
  - name: "yarn"
    config:
      scaling_factor: 2.0
      original_max_position_embeddings: 2048
      attention_factor: 0.1
      beta_fast: 32
      beta_slow: 1
  - name: "yarn"
    config:
      scaling_factor: 4.0
      original_max_position_embeddings: 2048
      attention_factor: 0.1
      beta_fast: 32
      beta_slow: 1
  - name: "longrope"
    config:
      scaling_factor: 2.0
      original_max_position_embeddings: 2048
      short_factor: [1.0, 1.0]
      long_factor: [2.0, 2.0]
  - name: "longrope"
    config:
      scaling_factor: 4.0
      original_max_position_embeddings: 2048
      short_factor: [1.0, 1.0]
      long_factor: [4.0, 4.0]
  - name: "dynamic_ntk"
    config:
      scaling_factor: 2.0
      original_max_position_embeddings: 2048
  - name: "dynamic_ntk"
    config:
      scaling_factor: 4.0
      original_max_position_embeddings: 2048
  - name: "llama3"
    config:
      scaling_factor: 8.0
      low_freq_factor: 1.0
      high_freq_factor: 4.0
      original_max_position_embeddings: 8192

# Evaluation Configuration
evaluation:
  batch_size: 1
  max_new_tokens: 100
  temperature: 0.0
  do_sample: false
  num_return_sequences: 1
  pad_token_id: null
  max_context_length: 32768
  gradient_checkpointing: true
  use_cache: false

# Data Configuration
data:
  cache_dir: "./cache/"
  output_dir: "./full_sweep_results/"

# Logging Configuration
logging:
  level: "INFO"
  file: "logs/full_sweep.log"

# Benchmark Configuration (all benchmarks)
benchmarks:
  niah:
    enabled: true
    variants: ["standard"]
    context_lengths: [4000, 8000, 16000, 32000]
    max_samples: 10
    
  ruler:
    enabled: true
    categories: ["retrieval", "qa"]
    max_length: 32000
    num_samples: 10
    
  longbench:
    enabled: true
    path: "data/longbench"
    tasks: ["narrativeqa", "qasper", "multifieldqa_en"]
    max_samples: 5
    
  longbench_v2:
    enabled: false  # Enable if you have LongBench v2 setup

# RoPE Extension Configuration (will be overridden by sweep)
rope_extension:
  method: "none"

# Hardware Configuration
hardware:
  num_gpus: 1
  gpu_memory_fraction: 0.9
  cpu_threads: 8
  mixed_precision: true
  compile_model: false