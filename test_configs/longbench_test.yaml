# LongBench Benchmark Test Configuration
# Tests the official LongBench implementation

# Model Configuration - lightweight for testing
model:
  type: "hf_hub"
  name: "llama-3.2-1b"
  path: "unsloth/Llama-3.2-1B"
  tokenizer_path: "unsloth/Llama-3.2-1B"
  device_map: "auto"
  torch_dtype: "bfloat16"
  attn_implementation: "eager"
  max_memory_gb: 24
  max_length: 8192
  trust_remote_code: false

# RoPE Methods - simple test
rope_methods:
  - name: "none"
    config: {}

# Evaluation Configuration
evaluation:
  batch_size: 1
  max_new_tokens: 100
  temperature: 0.0
  do_sample: false
  num_return_sequences: 1
  pad_token_id: null
  max_context_length: 8192
  gradient_checkpointing: true
  use_cache: false

# Data Configuration
data:
  cache_dir: "./cache/"
  output_dir: "./test_results/longbench/"

# Logging Configuration
logging:
  level: "INFO"
  file: "logs/longbench_test.log"

# LongBench Benchmark Configuration
benchmarks:
  longbench:
    enabled: true
    version: "v1"                              # Test LongBench v1
    dataset_name: "THUDM/LongBench"           # HuggingFace dataset
    tasks: ["narrativeqa", "qasper"]           # Test specific tasks
    max_samples: 2                             # Small number for quick test
    
  # Disable other benchmarks
  niah:
    enabled: false
  ruler:
    enabled: false
  longbench_v2:
    enabled: false

# RoPE Extension Configuration
rope_extension:
  method: "none"

# Hardware Configuration
hardware:
  num_gpus: 1
  gpu_memory_fraction: 0.9
  cpu_threads: 8
  mixed_precision: true
  compile_model: false