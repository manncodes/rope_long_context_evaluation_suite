# RULER Benchmark Test Configuration
# Tests the official RULER implementation

# Model Configuration - lightweight for testing
model:
  type: "hf_hub"
  name: "llama-3.2-1b"
  path: "unsloth/Llama-3.2-1B"
  tokenizer_path: "unsloth/Llama-3.2-1B"
  device_map: "auto"
  torch_dtype: "bfloat16"
  attn_implementation: "eager"
  max_memory_gb: 24
  max_length: 8192
  trust_remote_code: false

# RoPE Methods - simple test
rope_methods:
  - name: "none"
    config: {}

# Evaluation Configuration
evaluation:
  batch_size: 1
  max_new_tokens: 50
  temperature: 0.0
  do_sample: false
  num_return_sequences: 1
  pad_token_id: null
  max_context_length: 8192
  gradient_checkpointing: true
  use_cache: false

# Data Configuration
data:
  cache_dir: "./cache/"
  output_dir: "./test_results/ruler/"

# Logging Configuration
logging:
  level: "INFO"
  file: "logs/ruler_test.log"

# RULER Benchmark Configuration
benchmarks:
  ruler:
    enabled: true
    categories: ["niah_single_1", "vt", "cwe"]  # Test different RULER categories
    max_length: 8192                           # Maximum context length
    num_samples: 3                             # Small number for quick test
    sample_number_list: [1, 2, 3]             # Specific samples to test
    
  # Disable other benchmarks
  niah:
    enabled: false
  longbench:
    enabled: false
  longbench_v2:
    enabled: false

# RoPE Extension Configuration
rope_extension:
  method: "none"

# Hardware Configuration
hardware:
  num_gpus: 1
  gpu_memory_fraction: 0.9
  cpu_threads: 8
  mixed_precision: true
  compile_model: false