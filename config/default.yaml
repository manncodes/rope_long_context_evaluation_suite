# Default configuration for RoPE Long Context Evaluation Suite

# Model Configuration
model:
  # Model loading settings
  type: "hf_local"  # Options: hf_local, hf_hub, openai, anthropic
  name: "meta-llama/Llama-2-7b-hf"
  path: "./models/llama-2-7b-hf"
  tokenizer_path: "./models/llama-2-7b-hf"
  
  # Model parameters
  max_length: 4096
  device_map: "auto"
  torch_dtype: "auto"
  trust_remote_code: false
  
  # Quantization settings
  quantization:
    enabled: false
    method: "4bit"  # Options: 4bit, 8bit, fp16, bf16
    load_in_4bit: true
    load_in_8bit: false
    bnb_4bit_compute_dtype: "float16"
    bnb_4bit_use_double_quant: true
    bnb_4bit_quant_type: "nf4"

# RoPE Extension Methods
rope_extension:
  method: "linear_interpolation"  # Options: linear_interpolation, ntk_aware, yarn, longrope, dynamic_ntk
  
  # Linear Interpolation settings
  linear_interpolation:
    scaling_factor: 4
    
  # NTK-Aware settings
  ntk_aware:
    alpha: 1.0
    beta: 32.0
    
  # YaRN settings
  yarn:
    s: 16
    alpha: 1.0
    beta: 32.0
    attention_factor: 0.1
    beta_fast: 32.0
    beta_slow: 1.0
    
  # LongRoPE settings  
  longrope:
    short_factor: [1, 1, 1, 1, 1, 1, 1, 1]
    long_factor: [1, 1, 1, 1, 1, 1, 1, 1]
    
  # Dynamic NTK settings
  dynamic_ntk:
    alpha: 1.0

# Benchmark Configuration
benchmarks:
  # Needle in a Haystack (NIAH)
  niah:
    enabled: True
    variants: ['standard', 'multi_needle', 'nolib']
    context_lengths: ['4000', '8000', '16000', '32000', '64000', '128000', '256000']
    num_needles: 1
    depth_percent_intervals: [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]
    needle_file: "data/needles.txt"
    haystack_file: "data/haystack.txt"
    
  # RULER Benchmark
  ruler:
    enabled: True
    categories: ['retrieval', 'multi_hop', 'aggregation', 'qa']
    max_length: 128000
    num_samples: 500
    synthetic_tasks:
      - "niah_single_1"
      - "niah_single_2" 
      - "niah_single_3"
      - "niah_multikey_1"
      - "niah_multikey_2"
      - "niah_multikey_3"
      - "niah_multivalue"
      - "niah_multiquery"
      - "vt"
      - "cwe"
      - "fwe"
      - "qa_1"
      - "qa_2"
      
  # LongBench v2
  longbench_v2:
    enabled: True
    tasks: ['single_doc_qa', 'multi_doc_qa', 'long_icl', 'dialogue', 'code_repo', 'structured_data']
    data_path: "data/longbench_v2/"
    max_samples: 100
    
  # Original LongBench
  longbench:
    enabled: True
    tasks: ['narrativeqa', 'qasper', 'multifieldqa_en', 'hotpotqa', '2wikimqa', 'musique', 'gov_report', 'qmsum', 'multi_news', 'vcsum', 'trec', 'triviaqa', 'samsum', 'passage_count', 'passage_retrieval_en', 'lcc', 'repobench-p']
    data_path: "data/longbench/"
    max_samples: 100

# Data Configuration
data:
  cache_dir: "./cache/"
  output_dir: "./results/"
  temp_dir: "./temp/"
  download_datasets: true
  
# Evaluation Configuration  
evaluation:
  batch_size: 1
  num_workers: 4
  save_predictions: True
  compute_metrics: True
  generate_plots: True
  export_formats: ['json', 'csv', 'wandb']
  
  # Metrics to compute
  metrics:
    - "accuracy"
    - "f1"
    - "rouge"
    - "bleu"
    - "exact_match"
    
  # Generation parameters
  generation:
    max_new_tokens: 256
    temperature: 0.0
    do_sample: false
    top_p: 1.0
    top_k: 50
    repetition_penalty: 1.0
    length_penalty: 1.0
    num_beams: 1

# Logging and Monitoring
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: "logs/rope_eval.log"
  
  # Weights & Biases integration
  wandb:
    enabled: false
    project: "rope-long-context-eval"
    entity: null
    tags: []
    notes: ""
    
# Hardware Configuration
hardware:
  use_gpu: true
  gpu_ids: [0]
  mixed_precision: "fp16"  # Options: no, fp16, bf16
  gradient_checkpointing: false
  dataloader_num_workers: 4
  
# Reproducibility
seed: 42
deterministic: true