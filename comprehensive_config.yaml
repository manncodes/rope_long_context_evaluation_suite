# Comprehensive RoPE Long Context Evaluation Configuration
# Supports offline/NFS dataset paths and configurable model paths

# Model Configuration
model:
  name: "llama-3.2-1b"  # Used for results directory naming
  path: "unsloth/Llama-3.2-1B"  # Local path or HF model name - UPDATE THIS
  tokenizer_path: null  # Optional separate tokenizer path
  device: "cuda"
  torch_dtype: "bfloat16"  # Options: float16, bfloat16, float32
  attn_implementation: "flash_attention_2"  # Options: eager, flash_attention_2
  max_memory_gb: 24  # Max GPU memory to use

# Dataset Configuration (Offline/NFS paths)
datasets:
  longbench:
    path: "data/longbench"  # Path to LongBench JSONL files - UPDATE FOR YOUR SETUP
    tasks:
      - "narrativeqa"
      - "qasper" 
      - "multifieldqa_en"
      - "hotpotqa"
      - "2wikimqa"
      - "qmsum"
      - "trec"
      - "triviaqa"
      - "samsum"
      - "passage_retrieval_en"
      - "passage_count"
      - "lcc"
  
  # Traditional synthetic tasks (generated on-the-fly)
  retrieval:
    enabled: true
    context_lengths: [4000, 8000, 16000, 32000]
    num_samples: 50
  
  niah:
    enabled: true
    variants: ["standard", "multi_needle", "nolima"]
    context_lengths: [4000, 8000, 16000, 32000]
    num_samples: 20
  
  ruler:
    enabled: true
    tasks: ["retrieval", "multi_hop", "aggregation", "qa"]
    context_lengths: [4000, 8000, 16000, 32000] 
    num_samples: 20

# RoPE Scaling Methods to Evaluate
rope_methods:
  - name: "none"
    config: {}
  - name: "linear"
    config:
      scaling_factor: 2.0
  - name: "ntk_aware"
    config:
      scaling_factor: 2.0
      alpha: 8.0
  - name: "yarn"
    config:
      scaling_factor: 2.0
      original_max_position_embeddings: 2048
      attention_factor: 0.1
      beta_fast: 32
      beta_slow: 1
  - name: "longrope"
    config:
      scaling_factor: 2.0
      original_max_position_embeddings: 2048
      short_factor: [1.0, 1.0]
      long_factor: [2.0, 2.0]
  - name: "dynamic_ntk"
    config:
      scaling_factor: 2.0
      original_max_position_embeddings: 2048
  - name: "llama3"
    config:
      scaling_factor: 8.0
      low_freq_factor: 1.0
      high_freq_factor: 4.0
      original_max_position_embeddings: 8192

# Evaluation Configuration
evaluation:
  batch_size: 1  # Keep low for long context
  max_new_tokens: 100
  temperature: 0.0
  do_sample: false
  num_return_sequences: 1
  pad_token_id: null  # Will use model's pad token
  
  # Resource limits
  max_context_length: 32768  # Maximum context length to test
  gradient_checkpointing: true
  use_cache: false

# Output Configuration
output:
  base_dir: "comprehensive_results"
  save_predictions: true
  save_metrics: true
  create_visualizations: true
  
  # Results format
  formats: ["json", "csv"]
  
  # Logging
  log_level: "INFO"
  log_file: "evaluation.log"

# Hardware Configuration
hardware:
  num_gpus: 1
  gpu_memory_fraction: 0.9
  cpu_threads: 8
  mixed_precision: true
  compile_model: false  # Set to true for PyTorch 2.0+ optimization

# Experimental Features
experimental:
  use_flash_attention: true
  gradient_accumulation_steps: 1
  dataloader_num_workers: 4
  pin_memory: true